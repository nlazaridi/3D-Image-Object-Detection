{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notebook for working as a demo in the API deployment to add new features and/or fix bugs\n",
    "\n",
    "Most of the developments were done in the renderClass.py and then I was using this notebook for testing. \n",
    "\n",
    "NOTE: You need to use the example_client (or example_client_copy in my case).\n",
    "This is the client from the repository of the annotation service (https://github.com/gpan12/mv-annotation-service).\n",
    "Panagiotis Galopoulos can give you access so that you can clone it and import the client. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tempfile\n",
    "from pytorch3d.io import load_obj, load_ply\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from rendererClass import RendererClass\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "sys.path.append(\"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/Annotation_API/mv-annotation-service\")\n",
    "import example_client_copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import time\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import io\n",
    "import trimesh\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to test the API pipeline\n",
    "It can work with URL and local files\n",
    "URL is giving error when it comes to large gdrive files (because permission is needed)\n",
    "Therefore we make it work with local files as well for testing purposes (this is not part of the annotation API)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multiview_API_pipeline(obj_local_path, #path were the object is stored\n",
    "                           download_link, #url IF use_url true\n",
    "                           download_format, #format that the resuest has\n",
    "                           use_img # If tru use image directly\n",
    "                           ):\n",
    "\n",
    "    params = {\n",
    "            \"image_size\": 1024,\n",
    "            \"camera_dist\": [2],  \n",
    "            \"elevation\": [0],\n",
    "            \"azim_angle\": [0,45,90,135,180,225,270,315,360],\n",
    "            \"filename\": \"\",\n",
    "            \"file_extension\": \"glb\",\n",
    "            \"z_coord\": 0, \n",
    "            \"save_img\":False,                        \n",
    "            \"save_path\": \"\"\n",
    "            }\n",
    "\n",
    "    asset_type = params[\"file_extension\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "\n",
    "        # The context manager will automatically delete this directory after this section\n",
    "        logger.info(f\"Created a temporary directory: {tmpdir}\")\n",
    "\n",
    "\n",
    "        params[\"save_path\"] = tmpdir\n",
    "\n",
    "\n",
    "        if download_format == \"url\": #works with just one obj\n",
    "            print(f'Working with url, download from {download_link}')\n",
    "            obj = requests.get(download_link).content\n",
    "            \n",
    "        #reads the obj from local in order to get it in a similar format as requests.get()\n",
    "        elif download_format == \"local\": \n",
    "        \n",
    "            print(f'Working with local path, load from {obj_local_path}')\n",
    "            with open(obj_local_path, \"rb\") as object:\n",
    "                obj = object.read()\n",
    "            \n",
    "                temp_path =  os.path.normpath(obj_local_path)\n",
    "\n",
    "                src_dir = os.sep + os.path.join(*temp_path.split(os.sep)[:-1])\n",
    "                shutil.copytree(src_dir, tmpdir,dirs_exist_ok = True)\n",
    "        else:\n",
    "            raise ValueError(\"Need to chose between url, local or zip\")\n",
    "            \n",
    "        \n",
    "        annotations = []\n",
    "\n",
    "        \n",
    "        if asset_type == \"obj\":\n",
    "            with open(os.path.join(tmpdir,\"object.\" + asset_type),\"wb\") as f:\n",
    "                f.write(obj)\n",
    "            \n",
    "                params[\"filename\"] = f.name\n",
    "        #NOTE: very immature implementation\n",
    "        #TODO: update the condition with more efficient code\n",
    "        elif asset_type == \"zip\":\n",
    "            \n",
    "            \n",
    "            #---this is not needed but we want to work as the input in the service---\n",
    "            '''\n",
    "            destination_path = os.path.join(tmpdir,\"temp_zip.zip\")\n",
    "            print(f\"Zip folder to {destination_path}\")\n",
    "        \n",
    "            #NOTE: change this!\n",
    "            make_archive(os.path.dirname(obj_local_path), destination_path)\n",
    "            '''\n",
    "            # -------------------------------------------------------------------\n",
    "            \n",
    "            with zipfile.ZipFile(io.BytesIO(obj), 'r') as zip_ref:\n",
    "                zip_ref.extractall(tmpdir)\n",
    "                \n",
    "            for root, dirs, files in os.walk(tmpdir):\n",
    "                for file in files:\n",
    "                    \n",
    "                    _, file_ext = os.path.splitext(os.path.join(root,file))\n",
    "\n",
    "                    if file_ext == \".obj\":\n",
    "                        \n",
    "                        logger.info(\"I am in the zip dir\")\n",
    "                        \n",
    "                        params[\"filename\"] = os.path.join(root,file)\n",
    "                        params[\"file_extension\"] = \"obj\"\n",
    "                        \n",
    "        Renderer  = RendererClass(params)\n",
    "\n",
    "        logger.info(params)\n",
    "        Renderer.create_mesh_object(obj)\n",
    "            \n",
    "            \n",
    "        #load the pars to render 3D\n",
    "        all_dist = params[\"camera_dist\"]\n",
    "        all_elev = params[\"elevation\"]\n",
    "        all_azim = params[\"azim_angle\"]\n",
    "\n",
    "        #get all combinations\n",
    "        all_combs =  list(itertools.product(*[all_dist,all_elev,all_azim]))\n",
    "\n",
    "\n",
    "        logger.info(f'Number of images to annotate: {len(all_combs)}')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        #for loop to go through all the provided multiview images\n",
    "        for comb in all_combs:\n",
    "            \n",
    "            logger.info(f'Working with combination:{comb}')\n",
    "            \n",
    "            \n",
    "            \n",
    "            #store the image\n",
    "            img = Renderer.render_image(comb[0],comb[1],comb[2])\n",
    "            \n",
    "            path_to_store = os.path.join(tmpdir,str(comb)+\".png\")\n",
    "              \n",
    "                \n",
    "            logger.info(path_to_store)\n",
    "            matplotlib.image.imsave(path_to_store, img)    \n",
    "            \n",
    "            #return img\n",
    "            \n",
    "            \n",
    "            #define the asset path of the png\n",
    "            asset_path = path_to_store\n",
    "            #print(f'Asset path: {asset_path}')\n",
    "            logger.info(f'Asset path: {asset_path}')\n",
    "                 \n",
    "            \n",
    "            if use_img:\n",
    "            \n",
    "                logger.info(f'Work with img directly:')\n",
    "                \n",
    "                #convert numpy image to bytes\n",
    "                PIL_im = Image.fromarray((img * 255).astype(np.uint8))\n",
    "                img_byte_arr = io.BytesIO()\n",
    "                PIL_im.save(img_byte_arr, format='PNG')\n",
    "                img_byte_arr = img_byte_arr.getvalue()\n",
    "                \n",
    "                #return img,img_byte_arr\n",
    "                # Use the annotation client\n",
    "                try:\n",
    "                    resp = client.sync_annotate(\n",
    "                        asset_url=None,\n",
    "                        asset_path=None,\n",
    "                        asset_bytes = img_byte_arr,\n",
    "                        models = [\n",
    "                            example_client_copy.AnnotationModel.OBJECT_DETECTION,\n",
    "                        ],\n",
    "                        metadata={\"id\": \"1\"},\n",
    "                    )\n",
    "                    object_detection_result, error = resp.get_result(\n",
    "                        example_client_copy.AnnotationModel.OBJECT_DETECTION\n",
    "                    )\n",
    "\n",
    "                    if object_detection_result:\n",
    "                        \n",
    "                        # check is the specific model result is ok\n",
    "                        logger.info(object_detection_result)\n",
    "                        \n",
    "                except example_client_copy.AnnotationError:\n",
    "                    # Something pretty bad happened and all responses are lost.\n",
    "                    # This error should not happen under normal circumstances.\n",
    "                    # It's probably a bug that needs to be reported.\n",
    "                    logger.info(\"An error occurred\")\n",
    "        \n",
    "        \n",
    "        \n",
    "            else:\n",
    "                # Use the annotation client\n",
    "                try:\n",
    "                    resp = client.sync_annotate(\n",
    "                        asset_url=None,\n",
    "                        asset_path=asset_path,\n",
    "                        models = [\n",
    "                            example_client_copy.AnnotationModel.OBJECT_DETECTION,\n",
    "                        ],\n",
    "                        metadata={\"id\": \"1\"},\n",
    "                    )\n",
    "                    object_detection_result, error = resp.get_result(\n",
    "                        example_client_copy.AnnotationModel.OBJECT_DETECTION\n",
    "                    )\n",
    "\n",
    "                    if object_detection_result:\n",
    "                        \n",
    "                        # check is the specific model result is ok\n",
    "                        logger.info(object_detection_result)\n",
    "                        \n",
    "                except example_client_copy.AnnotationError:\n",
    "                    # Something pretty bad happened and all responses are lost.\n",
    "                    # This error should not happen under normal circumstances.\n",
    "                    # It's probably a bug that needs to be reported.\n",
    "                    logger.info(\"An error occurred\")\n",
    "            \n",
    "                \n",
    "            \n",
    "            #append the annotation list with new detections\n",
    "            [annotations.append(annot.object) for annot in object_detection_result.detections]\n",
    "            \n",
    "            logger.info(os.path.isdir(tmpdir))\n",
    "            \n",
    "    logger.info(os.path.isdir(tmpdir))       \n",
    "    logger.info(set(annotations))\n",
    "\n",
    "    return set(annotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  initialize client\n",
    "client = example_client_copy.MVAnnotationClient(\n",
    "        address=\"160.40.53.61:37527\",\n",
    "        secure=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.google.com/uc?export=download&id=1iNS4r9w4k8ekprVX1wAMYvB2_g-UdSLV\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/Police_Officer/UrbanPoliceOfficer.obj\"\n",
    "#obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/Astronaut/Astronaut.glb\"\n",
    "#obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/Apple/apples.gltf\"\n",
    "# obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/OFF_format/B.off\"\n",
    "# obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/OFF_format/7.off\"\n",
    "#obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/Liberty/Liberty_v4.glb\"\n",
    "obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/working_room/Darbo.obj\"\n",
    "#obj_local_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/data/random_example/Apple/apples.obj\"\n",
    "\n",
    "download_prefix = 'https://docs.google.com/uc?export=download&id='\n",
    "use_img = True\n",
    "download_format = \"url\"\n",
    "params_path = \"/home/andstasi/Projects/MediaVerse/3D_to_2D_converter/pytorch3d-renderer/params_inference.json\"\n",
    "\n",
    "link = \"https://drive.google.com/file/d/1MUdFiMGkjHi3IYWjaxDE8tLzNbgonBSF/view?usp=sharing\"\n",
    "link = \"https://drive.google.com/file/d/147WSr-W7ouf2vwoo-ZglFoE2R4qlf2Tp/view?usp=sharing\"\n",
    "#link = \"https://drive.google.com/drive/folders/1KwdSAzF097T5szuP8BAli0T7CZjFWrT0?usp=sharing\"\n",
    "link_astronaut = \"https://drive.google.com/file/d/1RwjwTdgGbeJR2U3VY6cx5DuEX0cjOFJn/view?usp=sharing\"\n",
    "link_astronaut_zip = \"https://drive.google.com/file/d/14KyaZUhKvGK4kvQtnx_4A-dzUUtoD_Zx/view?usp=sharing\"\n",
    "link_police_man = \"https://drive.google.com/file/d/1iNS4r9w4k8ekprVX1wAMYvB2_g-UdSLV/view?usp=sharing\"\n",
    "off_format = \"https://drive.google.com/file/d/1unNCZPLl0-nClFQgVHefKjFC1e3HymCW/view?usp=sharing\"\n",
    "link_apple_gltf = \"https://drive.google.com/file/d/1Ik4oi5nJGPnUDbgqyUX9qYOLPBFRvBD_/view?usp=sharing\"\n",
    "link_apple_obj = \"https://drive.google.com/file/d/1Y7Rr9rtKep4LdC3IS90-rJX9xgptTQao/view?usp=sharing\"\n",
    "linke_apple_zip = \"https://drive.google.com/file/d/1F7I7adr9o22CLUhYg3AuG6IrHdO59bSJ/view?usp=sharing\"\n",
    "link_tiger_ply = \"https://drive.google.com/file/d/1Vcsv0Rqym4LmOYqs1M97dXAgQu5p-jeR/view?usp=sharing\"\n",
    "link_single_bed_ply = \"https://drive.google.com/file/d/1moqu2Iy8HwTdNBW7XSYqgWeR2Iu8YqQl/view?usp=sharing\"\n",
    "link_single_bed_stl = \"https://drive.google.com/file/d/1WWLtRnWZnqyXo6cKN0pn0KVy-jQKzScd/view?usp=sharing\"\n",
    "link_single_bed_obj = \"https://drive.google.com/file/d/1KiCkOYxlESa9gJRe9BLO_SH9iFr5HRqL/view?usp=sharing\"\n",
    "link_full_body_ply = \"https://drive.google.com/file/d/1MjhKP-2P4ZYxh9WP1gwBGSFHRiCkUJXT/view?usp=sharing\"\n",
    "link_chair_obj = \"https://drive.google.com/file/d/1YTocHeQ-BX9dDh8q2qh6hQAxUUyMUlw6/view?usp=sharing\"\n",
    "link_char_ply = \"https://drive.google.com/file/d/1htM8gPxbxP1qRB0s_Pa3qcZMzYXpgkVE/view?usp=sharing\"\n",
    "link_apple_off = \"https://drive.google.com/file/d/1RNxgOiwJRmAfBeFu4-YzmRE_HEa0VZ3q/view?usp=sharing\"\n",
    "link_wm_glb = \"https://drive.google.com/file/d/1ulLCEFgA0jFskJC_l5XiKdDh404P2Slh/view?usp=sharing\"\n",
    "link_armchair_glb = \"https://drive.google.com/file/d/179DvBpKyQvvA46VsF9MxTVwY293NJXY_/view?usp=sharing\"\n",
    "link_sofa_glb = \"https://drive.google.com/file/d/1tSQAeIyOhu6rf1ulkbU6w0loI3CnVRUy/view?usp=sharing\"\n",
    "link_airplane_glb = \"https://drive.google.com/file/d/186N_o-eGlcK_qn2Q293MGbhCUDeXuKxq/view?usp=sharing\"\n",
    "link_kichennete_gltf = \"https://drive.google.com/file/d/1SfWFbqz60hCvThQa1hVwrrRkz_Cz2fDe/view?usp=sharing\"\n",
    "link_wooden_chair_glb = \"https://drive.google.com/file/d/1WAmlgEwVT9LSWarISOUhfQQUGpI6qITZ/view?usp=sharing\"\n",
    "link_mcclaren_glb = \"https://drive.google.com/file/d/1nhMtMI0rpHetFuFFP0Cx5H7GXzO8T97G/view?usp=sharing\"\n",
    "link_new_person_glb = \"https://drive.google.com/file/d/1Fb_DNg2Ps8AFxLfxXjZfL0jYhm2St1aI/view?usp=sharing\"\n",
    "download_link = download_prefix+link_police_man.strip().split(\"/\")[-2]\n",
    "#download_link = download_prefix+link.strip().split(\"/\")[-1]\n",
    "\n",
    "print(download_link)\n",
    "\n",
    "#img,img_byte_arr = multiview_API_pipeline(obj_local_path,download_link,use_url,use_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:40.022 | INFO     | __main__:multiview_API_pipeline:35 - Created a temporary directory: /tmp/tmptdad6aeb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with url, download from https://docs.google.com/uc?export=download&id=1Fb_DNg2Ps8AFxLfxXjZfL0jYhm2St1aI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:46.711 | INFO     | __main__:multiview_API_pipeline:100 - {'image_size': 1024, 'camera_dist': [2], 'elevation': [0], 'azim_angle': [0, 45, 90, 135, 180, 225, 270, 315, 360], 'filename': '', 'file_extension': 'glb', 'z_coord': 0, 'save_img': False, 'save_path': '/tmp/tmptdad6aeb'}\n",
      "2022-03-10 10:33:46.714 | INFO     | rendererClass:create_mesh_object:185 - Working with not OBJ format\n",
      "2022-03-10 10:33:46.736 | INFO     | rendererClass:create_mesh_object:198 - Created a temporary directory: /tmp/tmpbiw7afj9\n",
      "/home/andstasi/anaconda3/envs/triton_inf/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2022-03-10 10:33:51.266 | INFO     | rendererClass:create_mesh_object:231 - Working with atlas texture\n",
      "2022-03-10 10:33:51.272 | INFO     | __main__:multiview_API_pipeline:113 - Number of images to annotate: 9\n",
      "2022-03-10 10:33:51.272 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 0)\n",
      "2022-03-10 10:33:51.341 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 0).png\n",
      "2022-03-10 10:33:51.422 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 0).png\n",
      "2022-03-10 10:33:51.423 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize the input in a unit shpere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:53.575 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9993463158607483\n",
      "  bbox {\n",
      "    top_left_x: 0.12812581658363342\n",
      "    top_left_y: 0.3508036136627197\n",
      "    width: 0.9674507975578308\n",
      "    height: 0.6549289226531982\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:33:53.577 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:33:53.578 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 45)\n",
      "2022-03-10 10:33:53.648 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 45).png\n",
      "2022-03-10 10:33:53.721 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 45).png\n",
      "2022-03-10 10:33:53.721 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9993463158607483\n",
      "  bbox {\n",
      "    top_left_x: 0.12812581658363342\n",
      "    top_left_y: 0.3508036136627197\n",
      "    width: 0.9674507975578308\n",
      "    height: 0.6549289226531982\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:55.036 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9989985823631287\n",
      "  bbox {\n",
      "    top_left_x: 0.14269109070301056\n",
      "    top_left_y: 0.3907346725463867\n",
      "    width: 0.9536725282669067\n",
      "    height: 0.6206790208816528\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:33:55.037 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:33:55.039 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 90)\n",
      "2022-03-10 10:33:55.111 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 90).png\n",
      "2022-03-10 10:33:55.185 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 90).png\n",
      "2022-03-10 10:33:55.185 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9989985823631287\n",
      "  bbox {\n",
      "    top_left_x: 0.14269109070301056\n",
      "    top_left_y: 0.3907346725463867\n",
      "    width: 0.9536725282669067\n",
      "    height: 0.6206790208816528\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:56.465 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9974664449691772\n",
      "  bbox {\n",
      "    top_left_x: 0.15506352484226227\n",
      "    top_left_y: 0.37327855825424194\n",
      "    width: 0.9518532752990723\n",
      "    height: 0.6053697466850281\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:33:56.466 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:33:56.468 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 135)\n",
      "2022-03-10 10:33:56.534 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 135).png\n",
      "2022-03-10 10:33:56.630 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 135).png\n",
      "2022-03-10 10:33:56.631 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9974664449691772\n",
      "  bbox {\n",
      "    top_left_x: 0.15506352484226227\n",
      "    top_left_y: 0.37327855825424194\n",
      "    width: 0.9518532752990723\n",
      "    height: 0.6053697466850281\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:57.173 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9950173497200012\n",
      "  bbox {\n",
      "    top_left_x: 0.16390663385391235\n",
      "    top_left_y: 0.31979548931121826\n",
      "    width: 0.9309642910957336\n",
      "    height: 0.6161341667175293\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:33:57.174 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:33:57.176 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 180)\n",
      "2022-03-10 10:33:57.241 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 180).png\n",
      "2022-03-10 10:33:57.318 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 180).png\n",
      "2022-03-10 10:33:57.319 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9950173497200012\n",
      "  bbox {\n",
      "    top_left_x: 0.16390663385391235\n",
      "    top_left_y: 0.31979548931121826\n",
      "    width: 0.9309642910957336\n",
      "    height: 0.6161341667175293\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:58.641 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.999560534954071\n",
      "  bbox {\n",
      "    top_left_x: 0.1601981669664383\n",
      "    top_left_y: 0.35603344440460205\n",
      "    width: 0.9482297897338867\n",
      "    height: 0.6434560418128967\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:33:58.643 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:33:58.644 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 225)\n",
      "2022-03-10 10:33:58.714 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 225).png\n",
      "2022-03-10 10:33:58.786 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 225).png\n",
      "2022-03-10 10:33:58.787 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.999560534954071\n",
      "  bbox {\n",
      "    top_left_x: 0.1601981669664383\n",
      "    top_left_y: 0.35603344440460205\n",
      "    width: 0.9482297897338867\n",
      "    height: 0.6434560418128967\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:33:59.849 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.998615026473999\n",
      "  bbox {\n",
      "    top_left_x: 0.1685463786125183\n",
      "    top_left_y: 0.37495890259742737\n",
      "    width: 0.9857867956161499\n",
      "    height: 0.6225751638412476\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:33:59.850 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:33:59.852 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 270)\n",
      "2022-03-10 10:33:59.924 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 270).png\n",
      "2022-03-10 10:33:59.995 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 270).png\n",
      "2022-03-10 10:33:59.996 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.998615026473999\n",
      "  bbox {\n",
      "    top_left_x: 0.1685463786125183\n",
      "    top_left_y: 0.37495890259742737\n",
      "    width: 0.9857867956161499\n",
      "    height: 0.6225751638412476\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:34:00.744 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9977977275848389\n",
      "  bbox {\n",
      "    top_left_x: 0.1652575433254242\n",
      "    top_left_y: 0.38113635778427124\n",
      "    width: 0.9441201090812683\n",
      "    height: 0.6207900643348694\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:34:00.745 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:34:00.746 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 315)\n",
      "2022-03-10 10:34:00.812 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 315).png\n",
      "2022-03-10 10:34:00.888 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 315).png\n",
      "2022-03-10 10:34:00.889 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9977977275848389\n",
      "  bbox {\n",
      "    top_left_x: 0.1652575433254242\n",
      "    top_left_y: 0.38113635778427124\n",
      "    width: 0.9441201090812683\n",
      "    height: 0.6207900643348694\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:34:01.903 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9987704157829285\n",
      "  bbox {\n",
      "    top_left_x: 0.13354113698005676\n",
      "    top_left_y: 0.36973655223846436\n",
      "    width: 0.9250699281692505\n",
      "    height: 0.6983095407485962\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:34:01.904 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:34:01.906 | INFO     | __main__:multiview_API_pipeline:121 - Working with combination:(2, 0, 360)\n",
      "2022-03-10 10:34:01.973 | INFO     | __main__:multiview_API_pipeline:131 - /tmp/tmptdad6aeb/(2, 0, 360).png\n",
      "2022-03-10 10:34:02.052 | INFO     | __main__:multiview_API_pipeline:140 - Asset path: /tmp/tmptdad6aeb/(2, 0, 360).png\n",
      "2022-03-10 10:34:02.053 | INFO     | __main__:multiview_API_pipeline:145 - Work with img directly:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9987704157829285\n",
      "  bbox {\n",
      "    top_left_x: 0.13354113698005676\n",
      "    top_left_y: 0.36973655223846436\n",
      "    width: 0.9250699281692505\n",
      "    height: 0.6983095407485962\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 10:34:03.090 | INFO     | __main__:multiview_API_pipeline:172 - success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9993464350700378\n",
      "  bbox {\n",
      "    top_left_x: 0.128122478723526\n",
      "    top_left_y: 0.35080283880233765\n",
      "    width: 0.9674482345581055\n",
      "    height: 0.6549304127693176\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-03-10 10:34:03.091 | INFO     | __main__:multiview_API_pipeline:213 - True\n",
      "2022-03-10 10:34:03.094 | INFO     | __main__:multiview_API_pipeline:215 - False\n",
      "2022-03-10 10:34:03.096 | INFO     | __main__:multiview_API_pipeline:216 - {'person'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: 1\n",
      "detections {\n",
      "  object: \"person\"\n",
      "  confidence: 0.9993464350700378\n",
      "  bbox {\n",
      "    top_left_x: 0.128122478723526\n",
      "    top_left_y: 0.35080283880233765\n",
      "    width: 0.9674482345581055\n",
      "    height: 0.6549304127693176\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'person'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiview_API_pipeline(obj_local_path, #path were the object is stored\n",
    "                           download_link, #url IF use_url true\n",
    "                           download_format, #format that the resuest has\n",
    "                           use_img # If tru use image directly\n",
    "                           )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb3aa10cdef9f65946eadaea8cfe9157d932ff8335895c9f38c75282049245de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('triton_inf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
